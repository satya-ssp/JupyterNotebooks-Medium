{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_Internal_Lab_UpdatedTF2_Prices_Iris.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satya-ssp/JupyterNotebooks-Medium/blob/master/R6_Internal_Lab_UpdatedTF2_Prices_Iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84Q8JfvaeZZ6"
      },
      "source": [
        "## Linear Classifier in TensorFlow \n",
        "Using Low Level API in Eager Execution mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sb7Epo0VOB58"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHpCNRv1OB5-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mjtb-EMcm5K0",
        "colab": {}
      },
      "source": [
        "#Enable Eager Execution if using tensflow version < 2.0\n",
        "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DxJDmJqqOB6K"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhllFLyKOB6N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "674daf58-4cd1-4f69-fcb9-903d1cb3fc35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KiObW4V4SIOz",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_b0GSDhbtQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_pricing = pd.read_csv('/content/drive/My Drive/AIML 2019/prices.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgkX6SEqOB6W"
      },
      "source": [
        "### Check all columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7K8pWsNQOB6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "40a935f2-561b-4bf2-ef64-8ce05ea4fc65"
      },
      "source": [
        "data_pricing.info()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 851264 entries, 0 to 851263\n",
            "Data columns (total 7 columns):\n",
            "date      851264 non-null object\n",
            "symbol    851264 non-null object\n",
            "open      851264 non-null float64\n",
            "close     851264 non-null float64\n",
            "low       851264 non-null float64\n",
            "high      851264 non-null float64\n",
            "volume    851264 non-null float64\n",
            "dtypes: float64(5), object(2)\n",
            "memory usage: 45.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7dU6X7MpOB6c"
      },
      "source": [
        "### Drop columns `date` and  `symbol`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lh_6spSKOB6e",
        "colab": {}
      },
      "source": [
        "data_pricing_new = data_pricing.drop(['date','symbol'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlwbUgTwOB6i",
        "outputId": "2d3337b4-f289-4382-d17d-e21ceded1a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data_pricing_new.head()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DBv3WWYOB6q"
      },
      "source": [
        "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
        "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTCHJC2IcvMc",
        "colab_type": "text"
      },
      "source": [
        "data['volume'] = data['volume'] / 1000000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8omKwu9cYbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_pricing_new['volume'] = data_pricing_new['volume'].apply(lambda x: x/1000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaYqphbjdGXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pricing = data_pricing_new.head(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M3UaApqYOB6x"
      },
      "source": [
        "### Divide the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LE4U8lTdQJq",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X =  pricing.drop(\"volume\", axis=1)\n",
        "y =  pricing.pop(\"volume\")\n",
        "Xtrain,Xtest,ytrain,ytest = train_test_split(X, y, test_size=0.30, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oYK-aUuLbrz2"
      },
      "source": [
        "#### Convert Training and Test Data to numpy float32 arrays\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NobKcL-Odhpz",
        "colab_type": "text"
      },
      "source": [
        "import numpy as np\n",
        "x = np.float32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLPt0uxNdf42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "Xtrain = np.asarray(Xtrain)\n",
        "ytrain = np.asarray(ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF9yCcbde97n",
        "colab_type": "text"
      },
      "source": [
        "Input features\n",
        "x = tf.placeholder(shape=[None,13],dtype=tf.float32, name='x-input')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "im1ZegbDdKgv"
      },
      "source": [
        "### Normalize the data\n",
        "You can use Normalizer from sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A0LTL7rd5mk",
        "colab_type": "text"
      },
      "source": [
        "Normalize the data\n",
        "x_n = tf.nn.l2_normalize(x,1)\n",
        "\n",
        "Actual Prices\n",
        "y_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ37LlYVd1gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "X_normalized = preprocessing.normalize(X, norm='l2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v6vE4eYCOB62"
      },
      "source": [
        "## Building the Model in tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "297_qja4OB7A"
      },
      "source": [
        "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvwvzdj8eskY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input features\n",
        "x = tf.placeholder(shape=[None,4],dtype=tf.float32, name='x-input')\n",
        "\n",
        "#Normalize the data\n",
        "x_n = tf.nn.l2_normalize(x,1) \n",
        "\n",
        "#Actual Prices\n",
        "y_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')\n",
        "\n",
        "W = tf.Variable(tf.zeros(shape=[4,1]), name=\"Weights\")\n",
        "b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HgtWA-UIOB7F"
      },
      "source": [
        "2.Define a function to calculate prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKhe_29_fSNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = tf.add(tf.matmul(x_n,W),b,name='output') #X is normalized, hence X_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TL1hIwf_OB7M"
      },
      "source": [
        "3.Loss (Cost) Function [Mean square error]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VSWPiGXOB7P",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.square(y-y_),name='Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzG85FUlOB7U"
      },
      "source": [
        "4.Function to train the Model\n",
        "\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cj802w-3OB7X",
        "colab": {}
      },
      "source": [
        "train_op = tf.train.GradientDescentOptimizer(0.03).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSypb_u8OB7e"
      },
      "source": [
        "## Train the model for 100 epochs \n",
        "1. Observe the training loss at every iteration\n",
        "2. Observe Train loss at every 5th iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DVvgj7eQOB7f",
        "colab": {}
      },
      "source": [
        "#Lets start graph Execution\n",
        "sess = tf.Session()\n",
        "\n",
        "# variables need to be initialized before we can use them\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#how many times data need to be shown to model\n",
        "training_epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DOL2ncA1OB7q"
      },
      "source": [
        "### Get the shapes and values of W and b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZGvtyTeuOB7r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ac6d9ca5-7268-466a-cc27-6123f3c8ab76"
      },
      "source": [
        "print(W.shape)\n",
        "print(b.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 1)\n",
            "(1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ERq9GOKKciho"
      },
      "source": [
        "### Model Prediction on 1st Examples in Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKGvUWahcihp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "20ac21b6-42b9-480b-88e4-539313540109"
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "            \n",
        "    #Calculate train_op and loss\n",
        "    _, train_loss = sess.run([train_op,loss],feed_dict={x:Xtrain, y_:ytrain}) # X and Y is like X_Train and Y_Train here.\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "        print ('Training loss at step: ', epoch, ' is ', train_loss)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Training loss at step: ', 0, ' is ', 225.87958)\n",
            "('Training loss at step: ', 5, ' is ', 203.89143)\n",
            "('Training loss at step: ', 10, ' is ', 197.76749)\n",
            "('Training loss at step: ', 15, ' is ', 196.0619)\n",
            "('Training loss at step: ', 20, ' is ', 195.58687)\n",
            "('Training loss at step: ', 25, ' is ', 195.45459)\n",
            "('Training loss at step: ', 30, ' is ', 195.41774)\n",
            "('Training loss at step: ', 35, ' is ', 195.4075)\n",
            "('Training loss at step: ', 40, ' is ', 195.40462)\n",
            "('Training loss at step: ', 45, ' is ', 195.40382)\n",
            "('Training loss at step: ', 50, ' is ', 195.4036)\n",
            "('Training loss at step: ', 55, ' is ', 195.40355)\n",
            "('Training loss at step: ', 60, ' is ', 195.40353)\n",
            "('Training loss at step: ', 65, ' is ', 195.40355)\n",
            "('Training loss at step: ', 70, ' is ', 195.40353)\n",
            "('Training loss at step: ', 75, ' is ', 195.40353)\n",
            "('Training loss at step: ', 80, ' is ', 195.40353)\n",
            "('Training loss at step: ', 85, ' is ', 195.40353)\n",
            "('Training loss at step: ', 90, ' is ', 195.40355)\n",
            "('Training loss at step: ', 95, ' is ', 195.40353)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N12s78kgKEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3abb76c2-eb74-4915-86ec-ebb2a0d0bb8b"
      },
      "source": [
        "test_loss = sess.run([loss],feed_dict={x:Xtest, y_:ytest}) \n",
        "print ('Training loss at step:is ', test_loss)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Training loss at step:is ', [241.01949])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJRBuqXhOB7_"
      },
      "source": [
        "## Classification using tf.Keras\n",
        "\n",
        "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O0g6lorycihf"
      },
      "source": [
        "### Load the given Iris data using pandas (Iris.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6xFvb5sRcihg",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/My Drive/AIML 2019/11_Iris.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SAB--Qdwcihm"
      },
      "source": [
        "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CASrGfXgk8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_data = pd.get_dummies(data,prefix=['Species'], columns=['Species'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai-AfbZMg1Fw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7ce7e0d3-6474-4ee6-f3cb-eee57cc0b11a"
      },
      "source": [
        "k_data.info()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 8 columns):\n",
            "Id                         150 non-null int64\n",
            "SepalLengthCm              150 non-null float64\n",
            "SepalWidthCm               150 non-null float64\n",
            "PetalLengthCm              150 non-null float64\n",
            "PetalWidthCm               150 non-null float64\n",
            "Species_Iris-setosa        150 non-null uint8\n",
            "Species_Iris-versicolor    150 non-null uint8\n",
            "Species_Iris-virginica     150 non-null uint8\n",
            "dtypes: float64(4), int64(1), uint8(3)\n",
            "memory usage: 6.4 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fQ8wcmriEgB",
        "colab_type": "text"
      },
      "source": [
        "train_small_with_dummies = pd.get_dummies(train_small, sparse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D95nY5ILcihj"
      },
      "source": [
        "### Splitting the data into feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoxZ8Z0tiBko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X =  k_data.drop(['Id',\"Species_Iris-setosa\",'Species_Iris-versicolor',\n",
        "       'Species_Iris-virginica'], axis=1)\n",
        "y =  k_data[['Species_Iris-setosa', 'Species_Iris-versicolor',\n",
        "       'Species_Iris-virginica']]\n",
        "Xtrain,Xtest,ytrain,ytest = train_test_split(X, y, test_size=0.30, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2uCSaviMjD",
        "colab_type": "text"
      },
      "source": [
        "(features, actual_prices),_ = tf.keras.datasets.boston_housing.load_data(test_split=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b22qpC5xcihr"
      },
      "source": [
        "###  Building Model in tf.keras\n",
        "\n",
        "Build a Linear Classifier model  <br>\n",
        "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
        "2. Apply Softmax on Dense Layer outputs <br>\n",
        "3. Use SGD as Optimizer\n",
        "4. Use categorical_crossentropy as loss function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DQlSCHFieWh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "23539078-c141-49a8-dbca-25f5d2d8ac57"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "model=MLPClassifier(hidden_layer_sizes=(4))\n",
        "model.fit(Xtrain,ytrain)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=4, learning_rate='constant',\n",
              "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
              "       validation_fraction=0.1, verbose=False, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhYSRdUniWKz",
        "colab_type": "text"
      },
      "source": [
        "#Initialize Sequential Graph (model)\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Normalize input data\n",
        "model.add(tf.keras.layers.BatchNormalization(input_shape=(13,)))\n",
        "\n",
        "#Add Dense layer for prediction - Keras declares weights and bias automatically\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "#Compile the model - add Loss and Gradient Descent optimizer\n",
        "model.compile(optimizer='sgd', loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T5FdzqIKcihw"
      },
      "source": [
        "### Model Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4qLEdHPscihx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "433914b9-e7bb-4663-dd0b-8ae131d840a8"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(3, input_dim=4, kernel_initializer='normal', activation='softmax'))\n",
        "#model.add(Dense(3, kernel_initializer='normal'))\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics= ['accuracy'])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 12:38:45.026407 139666332325760 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 12:38:45.028867 139666332325760 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 12:38:45.034034 139666332325760 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "W0721 12:38:45.046658 139666332325760 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0721 12:38:45.063688 139666332325760 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-SgSSdRcih5"
      },
      "source": [
        "### Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBgKZkhkcih6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "de3a720c-1638-480a-f21a-f7fd3588fac4"
      },
      "source": [
        "model.fit(Xtrain, ytrain, validation_data=(Xtest,ytest), epochs=10, batch_size=5)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, y)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 12:39:14.799396 139666332325760 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0721 12:39:14.838546 139666332325760 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/10\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.0331 - acc: 0.4571 - val_loss: 1.0304 - val_acc: 0.6000\n",
            "Epoch 2/10\n",
            "105/105 [==============================] - 0s 555us/step - loss: 0.9180 - acc: 0.6857 - val_loss: 0.9296 - val_acc: 0.6000\n",
            "Epoch 3/10\n",
            "105/105 [==============================] - 0s 559us/step - loss: 0.8388 - acc: 0.7429 - val_loss: 0.8087 - val_acc: 0.6000\n",
            "Epoch 4/10\n",
            "105/105 [==============================] - 0s 537us/step - loss: 0.7671 - acc: 0.7048 - val_loss: 0.7864 - val_acc: 0.6000\n",
            "Epoch 5/10\n",
            "105/105 [==============================] - 0s 545us/step - loss: 0.7309 - acc: 0.7429 - val_loss: 0.7138 - val_acc: 0.6000\n",
            "Epoch 6/10\n",
            "105/105 [==============================] - 0s 523us/step - loss: 0.6817 - acc: 0.7429 - val_loss: 0.6660 - val_acc: 0.9556\n",
            "Epoch 7/10\n",
            "105/105 [==============================] - 0s 532us/step - loss: 0.6535 - acc: 0.7810 - val_loss: 0.6558 - val_acc: 0.6222\n",
            "Epoch 8/10\n",
            "105/105 [==============================] - 0s 531us/step - loss: 0.6214 - acc: 0.7333 - val_loss: 0.6152 - val_acc: 0.9778\n",
            "Epoch 9/10\n",
            "105/105 [==============================] - 0s 513us/step - loss: 0.5911 - acc: 0.8095 - val_loss: 0.6308 - val_acc: 0.6000\n",
            "Epoch 10/10\n",
            "105/105 [==============================] - 0s 552us/step - loss: 0.5754 - acc: 0.7714 - val_loss: 0.5790 - val_acc: 0.8222\n",
            "150/150 [==============================] - 0s 60us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "033PEdzAi1UR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0dca261b-6c74-4500-a156-eba95197301f"
      },
      "source": [
        "scores[1]*100"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82.00000015894572"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P32ASP1Vjt0a"
      },
      "source": [
        "### Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n8rd0jjAjyTR",
        "colab": {}
      },
      "source": [
        "model.save(\"model.dump\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XiipRpe7rbVh"
      },
      "source": [
        "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
        "\n",
        "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v5Du3lubr4sA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}